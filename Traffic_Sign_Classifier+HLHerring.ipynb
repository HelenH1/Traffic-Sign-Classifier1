{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Self-Driving Car Engineer Nanodegree\n",
    "Deep Learning\n",
    "Project: Build a Traffic Sign Recognition Classifier\n",
    "In this notebook, a template is provided for you to implement your functionality in stages, which is required to successfully complete this project. If additional code is required that cannot be included in the notebook, be sure that the Python code is successfully imported and included in your submission if necessary.\n",
    "Note: Once you have completed all of the code implementations, you need to finalize your work by exporting the iPython Notebook as an HTML document. Before exporting the notebook to html, all of the code cells need to have been run so that reviewers can see the final implementation and output. You can then export the notebook by using the menu above and navigating to \\n\", \"File -> Download as -> HTML (.html). Include the finished document along with this notebook as your submission.\n",
    "In addition to implementing code, there is a writeup to complete. The writeup should be completed in a separate file, which can be either a markdown file or a pdf document. There is a write up template that can be used to guide the writing process. Completing the code template and writeup template will cover all of the rubric points for this project.\n",
    "The rubric contains \"Stand Out Suggestions\" for enhancing the project beyond the minimum requirements. The stand out suggestions are optional. If you decide to pursue the \"stand out suggestions\", you can include the code in this Ipython notebook and also discuss the results in the writeup file.\n",
    "Note: Code and Markdown cells can be executed using the Shift + Enter keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode.\n",
    "![LeNet Architecture](lenet.png)\n",
    "Source: Yan LeCun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load Data\n",
    "\n",
    "Load the MNIST data, which comes pre-loaded with TensorFlow.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load pickled data\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# TODO: Fill this in based on where you saved the training and testing data\n",
    "\n",
    "training_file = 'traffic-signs-data/train_big.p'\n",
    "validation_file = 'traffic-signs-data/valid.p'\n",
    "testing_file = 'traffic-signs-data/test.p'\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_valid, y_valid = valid['features'], valid['labels']\n",
    "X_test, y_test = test['features'], test['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The MNIST data that TensorFlow pre-loads comes as 28x28x1 images.\n",
    "\n",
    "However, the LeNet architecture only accepts 32x32xC images, where C is the number of color channels.\n",
    "\n",
    "In order to reformat the MNIST data into a shape that LeNet will accept, we pad the data with two rows of zeros on the top and bottom, and two columns of zeros on the left and right (28+2+2 = 32).\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples = 107500\n",
      "Number of validation examples = 4410\n",
      "Number of testing examples = 12630\n",
      "Image data shape = (32, 32, 3)\n",
      "Number of classes = 43\n"
     ]
    }
   ],
   "source": [
    "### Replace each question mark with the appropriate value. \n",
    "### Use python, pandas or numpy methods rather than hard coding the results\n",
    "\n",
    "# TODO: Number of training examples\n",
    "n_train = len(X_train)\n",
    "\n",
    "# TODO: Number of validation examples\n",
    "n_validation = len(X_valid)\n",
    "\n",
    "# TODO: Number of testing examples.\n",
    "n_test = len(X_test)\n",
    "\n",
    "# TODO: What's the shape of an traffic sign image?\n",
    "image_shape = format(X_train[0].shape)\n",
    "\n",
    "# TODO: How many unique classes/labels there are in the dataset.\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of validation examples =\", n_validation)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Visualize Data\n",
    "\n",
    "View a sample from the dataset.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFsAAABZCAYAAABR/liSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfA/kRgEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110fea278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Data exploration visualization code goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline\n",
    "def showImage(): \n",
    "    index = random.randint(0, len(X_train))\n",
    "    image = X_train[index].squeeze()\n",
    "\n",
    "    plt.figure(figsize=(1,1))\n",
    "    plt.imshow(image)\n",
    "    print(y_train[index])\n",
    "    \n",
    "showImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nklSKwSFJKsXgkCSVYnBIkkoxOCRJpfx/ns1nlPQbs2oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109c566d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def showDistribution():\n",
    "    _, training_counts = np.unique(y_train, return_counts = True)\n",
    "    _, test_counts = np.unique(y_test, return_counts = True)\n",
    "    plt.bar( np.arange( 43 ), training_counts, align='center',  color='b', label='Training Data')\n",
    "    #plt.bar( np.arange( 43 ), test_counts, align='center', color='g', label='Testing Data')\n",
    "    plt.xlabel('ClassID')\n",
    "    plt.ylabel('Counts')\n",
    "    plt.xlim([-1, 43])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "showDistribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Extend Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Change the brightness of an image somewhere between 50% and 150%\n",
    "def change_brightness(img):\n",
    "    brightness = np.random.uniform(0.5, 1.5)\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_RGB2HSV)\n",
    "    img = np.array(img, dtype = np.float64)\n",
    "    img[:,:,2] = img[:,:,2] * brightness\n",
    "    # For over-large values, set to 255\n",
    "    img[:,:,2][img[:,:,2]>255]  = 255\n",
    "    # Convert back to uint8, then back to RGB\n",
    "    img = np.array(img, dtype = np.uint8)\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_HSV2RGB)\n",
    "    return img\n",
    "\n",
    "# Apply a translation to an image\n",
    "def change_translation(img, max_trans):\n",
    "    rows, cols = img.shape[:2]\n",
    "    #create a translation matrix, apply to image\n",
    "    trans_x = np.random.uniform(-max_trans, max_trans)\n",
    "    trans_y = np.random.uniform(-max_trans, max_trans)\n",
    "    trans_matrix = np.float32([[1, 0, trans_x], [0, 1, trans_y]])\n",
    "    img_trans = cv2.warpAffine(img, trans_matrix, (cols, rows))\n",
    "    return img_trans\n",
    "\n",
    "# Apply an affine transformation to an image\n",
    "def change_transformation(img, max_trans):\n",
    "    rows, cols = img.shape[:2]\n",
    "    # Define the starting point\n",
    "    pts1 = np.float32([[5, 5], [20, 5], [5, 20]])\n",
    "    # Calculate the random transformation\n",
    "    trans_p1 = 5 + np.random.uniform(-max_trans, max_trans)\n",
    "    trans_p2 = 20 + np.random.uniform(-max_trans, max_trans)\n",
    "    # Define the end point\n",
    "    pts2 = np.float32([[trans_p1, 5], [trans_p2, trans_p1], [5, trans_p2]])\n",
    "    # Calculate and apply the affine transformation matrix\n",
    "    trans_matrix = cv2.getAffineTransform(pts1, pts2)\n",
    "    img_trans = cv2.warpAffine(img, trans_matrix, (cols, rows))\n",
    "    return img_trans\n",
    "\n",
    "# Apply noise to the image\n",
    "def add_noise(img, amount):\n",
    "    num_light = np.ceil(amount * img.size * 0.5)\n",
    "    coords = [np.random.randint(0, i - 1, int(num_light))\n",
    "              for i in image.shape]\n",
    "    img[coords] = 1\n",
    "\n",
    "    num_dark = np.ceil(amount * image.size * 0.5)\n",
    "    coords = [np.random.randint(0, i - 1, int(num_dark))\n",
    "          for i in image.shape]\n",
    "    img[coords] = 0\n",
    "\n",
    "    # Return image\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Use the methods above to get up to 2000 samples per class in the training set. This takes ages - only do it once, then save the data for future use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Store the extended training dataset here\n",
    "X_train_big = np.empty([0, X_train.shape[1], X_train.shape[2], X_train.shape[3]], dtype = X_train.dtype)\n",
    "y_train_big = np.empty([0], dtype = y_train.dtype)\n",
    "\n",
    "# Position marker\n",
    "pos = 0\n",
    "\n",
    "# New class size\n",
    "class_size_n = 2500\n",
    "\n",
    "# Get counts per class\n",
    "_, class_counts = np.unique(y_train, return_counts = True)\n",
    "\n",
    "# Go through all possible classes and generate additional samples\n",
    "for c in range(n_classes):\n",
    "    # First copy existing data for this class\n",
    "    X_train_big = np.append(X_train_big, X_train[y_train == c], axis = 0)\n",
    "    y_train_big = np.append(y_train_big, y_train[y_train == c], axis = 0)\n",
    "    copied = len(X_train[y_train == c])\n",
    "    print('Class size is {}'.format(class_size_n))\n",
    "    print('Class count is {}'.format(class_counts[c]))\n",
    "    # Generate so many samples, that we have 2500 samples at the end per class\n",
    "    for i in range(class_size_n - class_counts[c]):\n",
    "        # Pick a random image from that class\n",
    "        number = np.random.randint(pos, pos + copied)\n",
    "        image = np.copy(X_train_big[number])\n",
    "        \n",
    "        # Apply one of those four image augmentations randomly\n",
    "        if (i % 4) == 0:\n",
    "            image = change_brightness(image)\n",
    "        elif (i % 4) == 1:\n",
    "            image = change_translation(image, 4)\n",
    "        elif (i % 4) == 2:\n",
    "            image = change_transformation(image, 2)\n",
    "        else:\n",
    "            image = add_noise(image, 0.01)\n",
    "            \n",
    "        # Reshape the image\n",
    "        image = image.reshape(1, 32, 32, 3)\n",
    "        \n",
    "        # Add augmented image to the end of the array\n",
    "        y_train_big = np.append(y_train_big, [c])\n",
    "        X_train_big = np.append(X_train_big, image, axis = 0)\n",
    "        \n",
    "        \n",
    "    # Copy current label\n",
    "    #y_train_big = np.append(y_train_big, np.full(len(X_train_big), c, dtype = int))\n",
    "        \n",
    "    # Raise position marker\n",
    "    pos += class_size_n\n",
    "    #print(len(y_train_big))\n",
    "    #print(len(X_train_big))\n",
    "    assert(len(X_train_big) == len(y_train_big))\n",
    "    \n",
    "    # Shows message\n",
    "    print(\"Finished class {}!\".format(c))\n",
    "    \n",
    "print(\"Data augmentation finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "assert(len(X_train_big) == len(y_train_big))\n",
    "import os\n",
    "      \n",
    "pickle_file = 'traffic-signs-data/train_big.p'\n",
    "if os.path.isfile(pickle_file):\n",
    "    print('Saving extended training data to pickle file...')\n",
    "    try:\n",
    "        with open(pickle_file, 'wb') as pfile:\n",
    "            pickle.dump(\n",
    "                {\n",
    "                    'features': X_train_big,\n",
    "                    'labels': y_train_big\n",
    "                },\n",
    "                pfile, 2)\n",
    "    except Exception as e:\n",
    "        print('Unable to save data to', pickle_file, ':', e)\n",
    "        raise\n",
    "\n",
    "print('Extended data training set saved in pickle file!')\n",
    "length = len(X_train_big)\n",
    "print('Pickle file has {} samples!'.format(length))\n",
    "assert(len(X_train_big) == len(y_train_big))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Revisualize Data\n",
    "Load the file, visualize a picture and show the distribution of the data again, to make sure it's now looking right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pickle_file = 'traffic-signs-data/train_big.p'\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  pickle_data = pickle.load(f)\n",
    "  X_train = pickle_data['features']\n",
    "  y_train = pickle_data['labels']\n",
    "  #del pickle_data  # Free up memory\n",
    "\n",
    "n_train = len(y_train)\n",
    "print('Extended data training set with {} samples loaded'.format(n_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nklSKwSFJKsXgkCSVYnBIkkoxOCRJpfx/ns1nlPQbs2oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109bb0e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "showDistribution()\n",
    "assert(len(X_train) == len(y_train))\n",
    "assert(len(X_valid) == len(y_valid))\n",
    "assert(len(X_test) == len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess Data\n",
    "\n",
    "Shuffle the training data.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/helenherring/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/skimage/util/dtype.py:110: UserWarning: Possible precision loss when converting from float64 to uint16\n",
      "  \"%s to %s\" % (dtypeobj_in, dtypeobj))\n",
      "/Users/helenherring/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/skimage/exposure/exposure.py:307: RuntimeWarning: invalid value encountered in true_divide\n",
      "  image = (image - imin) / float(imax - imin)\n"
     ]
    }
   ],
   "source": [
    "### Preprocess the data here. It is required to normalize the data. Other preprocessing steps could include \n",
    "### converting to grayscale, etc.\n",
    "### Feel free to use as many code cells as needed.\n",
    "from skimage import exposure, img_as_uint, data, img_as_ubyte, img_as_float\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "for image in X_train: \n",
    "    adapted = exposure.equalize_adapthist(image)\n",
    "    image = exposure.rescale_intensity(adapted) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images converted to grayscale!\n"
     ]
    }
   ],
   "source": [
    "### Define your architecture here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "import cv2\n",
    "\n",
    "def convertToGray(image_data):\n",
    "    image_data = cv2.cvtColor(image_data, cv2.COLOR_RGB2GRAY)\n",
    "    return image_data\n",
    "\n",
    "#X_train = np.array([convertToGray(X_train[i]) for i in range(len(X_train))])\n",
    "#X_valid = np.array([convertToGray(X_valid[i]) for i in range(len(X_valid))])\n",
    "#X_test = np.array([convertToGray(X_test[i]) for i in range(len(X_test))])\n",
    "\n",
    "\n",
    "print(\"Images converted to grayscale!\")\n",
    "\n",
    "\n",
    "    \n",
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Setup TensorFlow\n",
    "The `EPOCH` and `BATCH_SIZE` values affect the training speed and model accuracy.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 128\n",
    "keep_prob = 0.5\n",
    "learn_rate = 0.001\n",
    "\n",
    "# Run mode, set to test later\n",
    "run_mode = \"TRAIN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## SOLUTION: Implement LeNet-5\n",
    "Implement the [LeNet-5](http://yann.lecun.com/exdb/lenet/) neural network architecture.\n",
    "\n",
    "This is the only cell you need to edit.\n",
    "### Input\n",
    "The LeNet architecture accepts a 32x32xC image as input, where C is the number of color channels. Since MNIST images are grayscale, C is 1 in this case.\n",
    "\n",
    "### Architecture\n",
    "**Layer 1: Convolutional.** The output shape should be 28x28x6.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Pooling.** The output shape should be 14x14x6.\n",
    "\n",
    "**Layer 2: Convolutional.** The output shape should be 10x10x16.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Pooling.** The output shape should be 5x5x16.\n",
    "\n",
    "**Flatten.** Flatten the output shape of the final pooling layer such that it's 1D instead of 3D. The easiest way to do is by using `tf.contrib.layers.flatten`, which is already imported for you.\n",
    "\n",
    "**Layer 3: Fully Connected.** This should have 120 outputs.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Layer 4: Fully Connected.** This should have 84 outputs.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Layer 5: Fully Connected (Logits).** This should have 10 outputs.\n",
    "\n",
    "### Output\n",
    "Return the result of the 2nd fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image data shape = (32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Image data shape =\", image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def LeNet(x):    \n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "        \n",
    "    # Layer 1: Convolutional. Input = 32x32x3. Output = 28x28x32   \n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 3, 6), mean = mu, stddev = sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(6))\n",
    "    \n",
    "    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "    \n",
    "    # Activation and dropout. \n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    if run_mode is \"TRAIN\":\n",
    "        conv1 = tf.nn.dropout(conv1, keep_prob)\n",
    "        \n",
    "        \n",
    "    print(\"Made it through conv1!\")\n",
    "        \n",
    "    # Layer 2: 2nd Convolutional. Output: 28x28x6\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma))\n",
    "    conv2_b = tf.Variable(tf.zeros(16))\n",
    "    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "    \n",
    "    # Activation, pooling and dropout. Output = 14x14x64\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    if run_mode is \"TRAIN\":\n",
    "        conv2 = tf.nn.dropout(conv2, keep_prob)\n",
    "        \n",
    "    print(\"Made it through conv2!\")\n",
    "    \n",
    "    # Layer 3: 3rd Convolutional.\n",
    "    conv3_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma))\n",
    "    conv3_b = tf.Variable(tf.zeros(16))\n",
    "    conv3   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv3_b\n",
    "        \n",
    "    # Activation, pooling and dropout. \n",
    "    conv3 = tf.nn.relu(conv3)\n",
    "    conv3 = tf.nn.max_pool(conv3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    if run_mode is \"TRAIN\":\n",
    "        conv3 = tf.nn.dropout(conv3, keep_prob)\n",
    "    print(\"Made it through conv3!\")\n",
    "        \n",
    "    # Flatten. \n",
    "    fc0   = flatten(conv3)\n",
    "    \n",
    "    # Layer 4: Fully Connected with activation and dropout. \n",
    "    fc1_W = tf.Variable(tf.truncated_normal(shape=(2304, 120), mean = mu, stddev = sigma))\n",
    "    fc1_b = tf.Variable(tf.zeros(120))\n",
    "    fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "    if run_mode is \"TRAIN\":\n",
    "        fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "        \n",
    "    # SOLUTION: Activation.\n",
    "    fc1    = tf.nn.relu(fc1)\n",
    "    \n",
    "    print(\"Made it through fc1!\")\n",
    "\n",
    "    # Layer 5: Fully Connected with activation and dropout. \n",
    "    fc2_W  = tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma))\n",
    "    fc2_b  = tf.Variable(tf.zeros(84))\n",
    "    fc2    = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    if run_mode is \"TRAIN\":\n",
    "        fc2 = tf.nn.dropout(fc2, keep_prob)\n",
    "    fc2    = tf.nn.relu(fc2)\n",
    "    \n",
    "    print(\"Made it through fc2!\")\n",
    "    \n",
    "    # Layer 6: Fully Connected. \n",
    "    fc3_W  = tf.Variable(tf.truncated_normal(shape=(84, 43), mean = mu, stddev = sigma))\n",
    "    fc3_b  = tf.Variable(tf.zeros(43))\n",
    "    logits = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "    \n",
    "    print(\"Made it through fc3!\")\n",
    "        \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Features and Labels\n",
    "Train LeNet to classify [MNIST](http://yann.lecun.com/exdb/mnist/) data.\n",
    "\n",
    "`x` is a placeholder for a batch of input images.\n",
    "`y` is a placeholder for a batch of output labels.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training Pipeline\n",
    "Create a training pipeline that uses the model to classify MNIST data.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made it through conv1!\n",
      "Made it through conv2!\n",
      "Made it through conv3!\n",
      "Made it through fc1!\n",
      "Made it through fc2!\n",
      "Made it through fc3!\n",
      "Run through modified LeNet is completed!\n",
      "Made it through cross entropy and training loss calculation!\n",
      "Made it through the supporting pieces for evaluation!\n",
      "Made it through the full training pipeline!\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "\n",
    "h_fc1_drop = tf.nn.dropout(x, keep_prob)\n",
    "one_hot_y = tf.one_hot(y, 43)\n",
    "rate = learn_rate\n",
    "\n",
    "# Get  logits from neural network\n",
    "logits = LeNet(x)\n",
    "print(\"Run through modified LeNet is completed!\")\n",
    "\n",
    "# Calculate cross entropy from logits and one-hot labels\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = one_hot_y)\n",
    "#cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
    "\n",
    "# Calculate training loss\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "\n",
    "print(\"Made it through cross entropy and training loss calculation!\")\n",
    "\n",
    "# Define the training operation - minimize the optimizer\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "\n",
    "\n",
    "\n",
    "# Stuff for evaluation\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "print(\"Made it through the supporting pieces for evaluation!\")\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    print(\"Made it through the evaluation function!\")\n",
    "    return total_accuracy / num_examples\n",
    "\n",
    "# The accuracy measured against the validation set\n",
    "validation_accuracy = 0.0\n",
    "\n",
    "# Measurements use for graphing loss and accuracy\n",
    "epoch_log = []\n",
    "loss_epoch = []\n",
    "train_acc_epoch = []\n",
    "valid_acc_epoch = []\n",
    "\n",
    "print(\"Made it through the full training pipeline!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model Evaluation\n",
    "Evaluate how well the loss and accuracy of the model for a given dataset.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Model\n",
    "Run the training data through the training pipeline to train the model.\n",
    "\n",
    "Before each epoch, shuffle the training set.\n",
    "\n",
    "After each epoch, measure the loss and accuracy of the validation set.\n",
    "\n",
    "Save the model after training.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 100 epochs with 840 batches/epoch and 128 samples/batch\n",
      "\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 0:\n",
      "Training accuracy = 0.024\n",
      "Validation accuracy = 0.015\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 1:\n",
      "Training accuracy = 0.024\n",
      "Validation accuracy = 0.015\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 2:\n",
      "Training accuracy = 0.024\n",
      "Validation accuracy = 0.013\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 3:\n",
      "Training accuracy = 0.025\n",
      "Validation accuracy = 0.048\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 4:\n",
      "Training accuracy = 0.046\n",
      "Validation accuracy = 0.075\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 5:\n",
      "Training accuracy = 0.061\n",
      "Validation accuracy = 0.069\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 6:\n",
      "Training accuracy = 0.094\n",
      "Validation accuracy = 0.131\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 7:\n",
      "Training accuracy = 0.135\n",
      "Validation accuracy = 0.173\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 8:\n",
      "Training accuracy = 0.156\n",
      "Validation accuracy = 0.193\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 9:\n",
      "Training accuracy = 0.176\n",
      "Validation accuracy = 0.202\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 10:\n",
      "Training accuracy = 0.187\n",
      "Validation accuracy = 0.222\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 11:\n",
      "Training accuracy = 0.196\n",
      "Validation accuracy = 0.228\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 12:\n",
      "Training accuracy = 0.212\n",
      "Validation accuracy = 0.238\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 13:\n",
      "Training accuracy = 0.226\n",
      "Validation accuracy = 0.260\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 14:\n",
      "Training accuracy = 0.239\n",
      "Validation accuracy = 0.252\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 15:\n",
      "Training accuracy = 0.247\n",
      "Validation accuracy = 0.259\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 16:\n",
      "Training accuracy = 0.261\n",
      "Validation accuracy = 0.265\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 17:\n",
      "Training accuracy = 0.259\n",
      "Validation accuracy = 0.264\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 18:\n",
      "Training accuracy = 0.292\n",
      "Validation accuracy = 0.289\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 19:\n",
      "Training accuracy = 0.321\n",
      "Validation accuracy = 0.313\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 20:\n",
      "Training accuracy = 0.340\n",
      "Validation accuracy = 0.344\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 21:\n",
      "Training accuracy = 0.364\n",
      "Validation accuracy = 0.358\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 22:\n",
      "Training accuracy = 0.349\n",
      "Validation accuracy = 0.360\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 23:\n",
      "Training accuracy = 0.376\n",
      "Validation accuracy = 0.387\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 24:\n",
      "Training accuracy = 0.402\n",
      "Validation accuracy = 0.373\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 25:\n",
      "Training accuracy = 0.413\n",
      "Validation accuracy = 0.414\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 26:\n",
      "Training accuracy = 0.431\n",
      "Validation accuracy = 0.416\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 27:\n",
      "Training accuracy = 0.406\n",
      "Validation accuracy = 0.384\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 28:\n",
      "Training accuracy = 0.451\n",
      "Validation accuracy = 0.429\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 29:\n",
      "Training accuracy = 0.458\n",
      "Validation accuracy = 0.429\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 30:\n",
      "Training accuracy = 0.469\n",
      "Validation accuracy = 0.455\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 31:\n",
      "Training accuracy = 0.472\n",
      "Validation accuracy = 0.450\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 32:\n",
      "Training accuracy = 0.484\n",
      "Validation accuracy = 0.464\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 33:\n",
      "Training accuracy = 0.483\n",
      "Validation accuracy = 0.477\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 34:\n",
      "Training accuracy = 0.494\n",
      "Validation accuracy = 0.470\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 35:\n",
      "Training accuracy = 0.493\n",
      "Validation accuracy = 0.464\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 36:\n",
      "Training accuracy = 0.498\n",
      "Validation accuracy = 0.478\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 37:\n",
      "Training accuracy = 0.502\n",
      "Validation accuracy = 0.472\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 38:\n",
      "Training accuracy = 0.482\n",
      "Validation accuracy = 0.453\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 39:\n",
      "Training accuracy = 0.498\n",
      "Validation accuracy = 0.476\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 40:\n",
      "Training accuracy = 0.514\n",
      "Validation accuracy = 0.478\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 41:\n",
      "Training accuracy = 0.502\n",
      "Validation accuracy = 0.480\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 42:\n",
      "Training accuracy = 0.512\n",
      "Validation accuracy = 0.483\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 43:\n",
      "Training accuracy = 0.518\n",
      "Validation accuracy = 0.493\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 44:\n",
      "Training accuracy = 0.536\n",
      "Validation accuracy = 0.496\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 45:\n",
      "Training accuracy = 0.526\n",
      "Validation accuracy = 0.489\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 46:\n",
      "Training accuracy = 0.538\n",
      "Validation accuracy = 0.514\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 47:\n",
      "Training accuracy = 0.532\n",
      "Validation accuracy = 0.494\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 48:\n",
      "Training accuracy = 0.527\n",
      "Validation accuracy = 0.503\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 49:\n",
      "Training accuracy = 0.547\n",
      "Validation accuracy = 0.526\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 50:\n",
      "Training accuracy = 0.538\n",
      "Validation accuracy = 0.519\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 51:\n",
      "Training accuracy = 0.544\n",
      "Validation accuracy = 0.519\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 52:\n",
      "Training accuracy = 0.517\n",
      "Validation accuracy = 0.476\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 53:\n",
      "Training accuracy = 0.556\n",
      "Validation accuracy = 0.521\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 54:\n",
      "Training accuracy = 0.561\n",
      "Validation accuracy = 0.536\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 55:\n",
      "Training accuracy = 0.559\n",
      "Validation accuracy = 0.531\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 56:\n",
      "Training accuracy = 0.565\n",
      "Validation accuracy = 0.529\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 57:\n",
      "Training accuracy = 0.566\n",
      "Validation accuracy = 0.522\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 58:\n",
      "Training accuracy = 0.546\n",
      "Validation accuracy = 0.513\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 59:\n",
      "Training accuracy = 0.558\n",
      "Validation accuracy = 0.511\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 60:\n",
      "Training accuracy = 0.535\n",
      "Validation accuracy = 0.488\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 61:\n",
      "Training accuracy = 0.574\n",
      "Validation accuracy = 0.543\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 62:\n",
      "Training accuracy = 0.571\n",
      "Validation accuracy = 0.529\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 63:\n",
      "Training accuracy = 0.573\n",
      "Validation accuracy = 0.544\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 64:\n",
      "Training accuracy = 0.575\n",
      "Validation accuracy = 0.554\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 65:\n",
      "Training accuracy = 0.575\n",
      "Validation accuracy = 0.559\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 66:\n",
      "Training accuracy = 0.579\n",
      "Validation accuracy = 0.554\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 67:\n",
      "Training accuracy = 0.574\n",
      "Validation accuracy = 0.540\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 68:\n",
      "Training accuracy = 0.580\n",
      "Validation accuracy = 0.551\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 69:\n",
      "Training accuracy = 0.575\n",
      "Validation accuracy = 0.530\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 70:\n",
      "Training accuracy = 0.587\n",
      "Validation accuracy = 0.559\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 71:\n",
      "Training accuracy = 0.585\n",
      "Validation accuracy = 0.548\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 72:\n",
      "Training accuracy = 0.574\n",
      "Validation accuracy = 0.548\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 73:\n",
      "Training accuracy = 0.579\n",
      "Validation accuracy = 0.551\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 74:\n",
      "Training accuracy = 0.590\n",
      "Validation accuracy = 0.565\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 75:\n",
      "Training accuracy = 0.581\n",
      "Validation accuracy = 0.556\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 76:\n",
      "Training accuracy = 0.589\n",
      "Validation accuracy = 0.581\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 77:\n",
      "Training accuracy = 0.575\n",
      "Validation accuracy = 0.551\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 78:\n",
      "Training accuracy = 0.590\n",
      "Validation accuracy = 0.568\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 79:\n",
      "Training accuracy = 0.586\n",
      "Validation accuracy = 0.563\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 80:\n",
      "Training accuracy = 0.577\n",
      "Validation accuracy = 0.548\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 81:\n",
      "Training accuracy = 0.593\n",
      "Validation accuracy = 0.566\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 82:\n",
      "Training accuracy = 0.599\n",
      "Validation accuracy = 0.576\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 83:\n",
      "Training accuracy = 0.572\n",
      "Validation accuracy = 0.553\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 84:\n",
      "Training accuracy = 0.540\n",
      "Validation accuracy = 0.502\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 85:\n",
      "Training accuracy = 0.589\n",
      "Validation accuracy = 0.576\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 86:\n",
      "Training accuracy = 0.598\n",
      "Validation accuracy = 0.577\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 87:\n",
      "Training accuracy = 0.600\n",
      "Validation accuracy = 0.582\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 88:\n",
      "Training accuracy = 0.598\n",
      "Validation accuracy = 0.568\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 89:\n",
      "Training accuracy = 0.601\n",
      "Validation accuracy = 0.576\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 90:\n",
      "Training accuracy = 0.600\n",
      "Validation accuracy = 0.579\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 91:\n",
      "Training accuracy = 0.573\n",
      "Validation accuracy = 0.534\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 92:\n",
      "Training accuracy = 0.597\n",
      "Validation accuracy = 0.571\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 93:\n",
      "Training accuracy = 0.602\n",
      "Validation accuracy = 0.583\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 94:\n",
      "Training accuracy = 0.602\n",
      "Validation accuracy = 0.581\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 95:\n",
      "Training accuracy = 0.599\n",
      "Validation accuracy = 0.585\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 96:\n",
      "Training accuracy = 0.601\n",
      "Validation accuracy = 0.574\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 97:\n",
      "Training accuracy = 0.584\n",
      "Validation accuracy = 0.574\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 98:\n",
      "Training accuracy = 0.602\n",
      "Validation accuracy = 0.571\n",
      "Made it through the evaluation function!\n",
      "Made it through the evaluation function!\n",
      "EPOCH 99:\n",
      "Training accuracy = 0.606\n",
      "Validation accuracy = 0.566\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'saver_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-ca8221aecd1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Save the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaver_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model saved into {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaver_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'saver_file' is not defined"
     ]
    }
   ],
   "source": [
    "### Run session\n",
    "import math\n",
    "\n",
    "with tf.Session() as session:\n",
    "    \n",
    "    # Init the weights and biases\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Calculate the number of batches per epoch\n",
    "    batch_count = int(math.ceil(n_train / BATCH_SIZE))\n",
    "\n",
    "    print(\"Training {} epochs with {} batches/epoch and {} samples/batch\".format(EPOCHS, batch_count, BATCH_SIZE))\n",
    "    print()\n",
    "    \n",
    "    # Go through the single epochs\n",
    "    for epoch_n in range(EPOCHS):\n",
    "        \n",
    "        # Shuffle training data every epoch\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        \n",
    "        # Go through the batches\n",
    "        for offset in range(0, n_train, BATCH_SIZE):\n",
    "            \n",
    "            # Load a new batch of data\n",
    "            batch_x, batch_y = X_train[offset : offset+BATCH_SIZE], y_train[offset : offset+BATCH_SIZE]\n",
    "            \n",
    "            # Run training operation\n",
    "            _, l = session.run([training_operation, loss_operation], feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "        # Show training and validation accuracy for every epoch\n",
    "        training_accuracy = evaluate(X_train, y_train)\n",
    "        validation_accuracy = evaluate(X_valid, y_valid)\n",
    "        print(\"EPOCH {}:\".format(epoch_n))\n",
    "        print(\"Training accuracy = {:.3f}\".format(training_accuracy))\n",
    "        print(\"Validation accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        \n",
    "         # Log epochs\n",
    "        previous_epoch = epoch_log[-1] if epoch_log else 0\n",
    "        epoch_log.append(1 + previous_epoch)\n",
    "        loss_epoch.append(l)\n",
    "        train_acc_epoch.append(training_accuracy)\n",
    "        valid_acc_epoch.append(validation_accuracy)\n",
    "        \n",
    "    # Save the model \n",
    "    saver.save(session, saver_file)\n",
    "    print()\n",
    "    print(\"Model saved into {}\".format(saver_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Step 3: Test a Model on New Images\n",
    "To give yourself more insight into how your model is working, download at least five pictures of German traffic signs from the web and use your model to predict the traffic sign type.\n",
    "You may find signnames.csv useful as it contains mappings from the class id (integer) to the actual sign name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Load the images and plot them here.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Run the predictions here and use the model to output the prediction for each image.\n",
    "### Make sure to pre-process the images with the same pre-processing pipeline used earlier.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Calculate the accuracy for these 5 new images. \n",
    "### For example, if the model predicted 1 out of 5 signs correctly, it's 20% accurate on these new images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Output Top 5 Softmax Probabilities For Each Image Found on the Web\n",
    "For each of the new images, print out the model's softmax probabilities to show the certainty of the model's predictions (limit the output to the top 5 probabilities for each image). tf.nn.top_k could prove helpful here.\n",
    "The example below demonstrates how tf.nn.top_k can be used to find the top k predictions for each image.\n",
    "tf.nn.top_k will return the values and indices (class ids) of the top k predictions. So if k=3, for each sign, it'll return the 3 largest probabilities (out of a possible 43) and the correspoding class ids.\n",
    "Take this numpy array as an example. The values in the array represent predictions. The array contains softmax probabilities for five candidate images with six possible classes. tk.nn.top_k is used to choose the three classes with the highest probability:\n",
    "# (5, 6) array\n",
    "a = np.array([[ 0.24879643,  0.07032244,  0.12641572,  0.34763842,  0.07893497,\n",
    "         0.12789202],\n",
    "       [ 0.28086119,  0.27569815,  0.08594638,  0.0178669 ,  0.18063401,\n",
    "         0.15899337],\n",
    "       [ 0.26076848,  0.23664738,  0.08020603,  0.07001922,  0.1134371 ,\n",
    "         0.23892179],\n",
    "       [ 0.11943333,  0.29198961,  0.02605103,  0.26234032,  0.1351348 ,\n",
    "         0.16505091],\n",
    "       [ 0.09561176,  0.34396535,  0.0643941 ,  0.16240774,  0.24206137,\n",
    "         0.09155967]])\n",
    "Running it through sess.run(tf.nn.top_k(tf.constant(a), k=3)) produces:\n",
    "TopKV2(values=array([[ 0.34763842,  0.24879643,  0.12789202],\n",
    "       [ 0.28086119,  0.27569815,  0.18063401],\n",
    "       [ 0.26076848,  0.23892179,  0.23664738],\n",
    "       [ 0.29198961,  0.26234032,  0.16505091],\n",
    "       [ 0.34396535,  0.24206137,  0.16240774]]), indices=array([[3, 0, 5],\n",
    "       [0, 1, 4],\n",
    "       [0, 5, 1],\n",
    "       [1, 3, 5],\n",
    "       [1, 4, 3]], dtype=int32))\n",
    "Looking just at the first row we get [ 0.34763842,  0.24879643,  0.12789202], you can confirm these are the 3 largest probabilities in a. You'll also notice [3, 0, 5] are the corresponding indices.\n",
    "In [3]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Print out the top five softmax probabilities for the predictions on the German traffic sign images found on the web. \n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Evaluate the Model\n",
    "Once you are completely satisfied with your model, evaluate the performance of the model on the test set.\n",
    "\n",
    "Be sure to only do this once!\n",
    "\n",
    "If you were to measure the performance of your trained model on the test set, then improve your model, and then measure the performance of your model on the test set again, that would invalidate your test results. You wouldn't get a true measure of how well your model would perform against real data.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "\n",
    "    test_accuracy = evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
